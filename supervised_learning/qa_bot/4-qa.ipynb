{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Based on the stuff done previously:\n",
    "* Writes a function that answers questions from multiple reference txt'\n",
    "\n",
    "As torch is currently not working on my system I can't show the outputs. These do work THO."
   ],
   "id": "8c655b10d476969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer"
   ],
   "id": "12d038cc7e4ff7a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def question_answer(corpus_path):\n",
    "    \"\"\"\n",
    "    Answers questions from multiple reference txt's\n",
    "    \n",
    "    Parameters:\n",
    "        corpus_path -> path of the corpus file\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:    \n",
    "        global A\n",
    "        A = input(\"Q: \")\n",
    "        exit = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
    "\n",
    "        if A.lower() in exit:\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        reference = semantic_search(corpus_path, A)\n",
    "        answer = question_answerer(A, reference)\n",
    "        \n",
    "        if answer is None:\n",
    "            print(\"A: Apologies, I do not understand the question given. Please try again.\")\n",
    "        else:\n",
    "            print(\"A: \", answer)\n",
    "\n",
    "def semantic_search(corpus_path, sentence):\n",
    "    \"\"\"\n",
    "    Performs semantic search on a corpus.\n",
    "    \n",
    "    Parameters:\n",
    "        corpus_path -> path to the corpus of reference documents.\n",
    "        sentence -> the sentence to be semantic searched.\n",
    "        \n",
    "    Returns:\n",
    "        the reference text similar to sentence.\n",
    "    \"\"\"\n",
    "    document = [sentence]\n",
    "    \n",
    "    # Loading and reading the data\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".md\") is False:\n",
    "            continue\n",
    "        with open(corpus_path + \"/\" + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            document.append(file.read())\n",
    "            \n",
    "    model = hub.load(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "    ) # This is the USE model from tensorflow hub. it is designed to semantically produce meaningful vectors\n",
    "    \n",
    "    embeddings = model(document)\n",
    "    \n",
    "    correlation = np.inner(embeddings, embeddings) # Finding Correlations in the embeddings of the document\n",
    "    \n",
    "    closest = np.argmax(correlation[0, 1:]) # Finding the closest document in similarity\n",
    "    \n",
    "    similar = document[closest + 1] # Find the most similar document of the bunch\n",
    "    \n",
    "    return similar\n",
    "\n",
    "def question_answerer(question, reference):\n",
    "    \"\"\"\n",
    "    Finds a snippet of text within a document in order to answer questions.\n",
    "    \"\"\"\n",
    "    token = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "    \n",
    "    question_tokens = token.tokenize(question)\n",
    "    reference_tokens = token.tokenize(reference)\n",
    "    \n",
    "    tokens = [\"[CLS]\"] + question_tokens + [\"[SEP]\"] + reference_tokens + [\"[SEP]\"]\n",
    "    \n",
    "    input_id = token.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_id)\n",
    "    input_type_ids = ([0] * (\n",
    "        1 + len(question_tokens) + 1) +\n",
    "                      [1] * (len(reference_tokens) + 1\n",
    "    ))\n",
    "    \n",
    "    input_id, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_id, input_mask, input_type_ids)\n",
    "        )\n",
    "    \n",
    "    outputs = model(input_ids=input_id, input_mask=input_mask, token_type_ids=input_type_ids)\n",
    "    \n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_token = tokens[short_start : short_end + 1]\n",
    "    answer = token.convert_tokens_to_string(answer_token)\n",
    "    \n",
    "    if answer == None or answer == \"\" or question in answer:\n",
    "        return None\n",
    "\n",
    "    return answer"
   ],
   "id": "3f2766006d74803a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
