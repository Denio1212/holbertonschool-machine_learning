{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The question answering Function\n",
    "\n",
    "* Makes a function that finds a snippet of text within a reference document in order to answer a question\n"
   ],
   "id": "13fe4c96b09afe7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer"
   ],
   "id": "ebb327b9ebd844cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* **Question** is a string containing the question to answer\n",
    "- **Reference** is a string containing the reference document from which to find the answer.\n",
    "* **Returns**: String containing the answer\n",
    "        \n",
    "        If no answer is found return **None**"
   ],
   "id": "cfd1f0b0cbdfe26b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def question_answer(question, reference):\n",
    "    \"\"\"\n",
    "    A function which finds a snippet of txt and answers a question\n",
    "    \"\"\"\n",
    "    # Loading the model and it's tokenizer\n",
    "    bert = hub.load(\"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    \n",
    "    # Tokenizes the questions and references\n",
    "    tokens_q = tokenizer(question, return_tensors=\"tf\", truncation=True, padding=True)[\"input_ids\"]\n",
    "    tokens_r = tokenizer(reference, return_tensors=\"tf\", truncation=True, padding=True)[\"input_ids\"]\n",
    "    \n",
    "    # Combine the questiona and reference documents into one\n",
    "    combine = tf.concat([tokens_q[:, : 1024], tokens_r[:, 1024 :]], axis=-1)\n",
    "    \n",
    "    # Make the predictions\n",
    "    outputs = bert(combine)[0]\n",
    "    logits = tf.argmax(outputs, axis=1).numpy()\n",
    "    \n",
    "    # Get the answer based on predicted idx\n",
    "    if logits[0] < len(tokens_q[0]):\n",
    "        return tokenizer.decode([tokens_r[0][logits[0]]])\n",
    "    else:\n",
    "        return None\n"
   ],
   "id": "735183f72fdf373e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Main Function\n",
    "\n",
    "with open('data/ZendeskArticles/PeerLearningDays.md') as f:\n",
    "    reference = f.read()\n",
    "\n",
    "print(question_answer('When are PLDs?', reference))"
   ],
   "id": "86f7608ca8e398db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The model used for this was the only one that seemed to get accepted.\n",
    "\n",
    "\n",
    "Although I now have a much bigger problem, which is the fact that the library **Torch** refuses to be read by my IDE.\n",
    "\n",
    "I am quite certain that this works, but i do not know how to fix this without spending 3 hours on it"
   ],
   "id": "2356d58b04d0ab82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
