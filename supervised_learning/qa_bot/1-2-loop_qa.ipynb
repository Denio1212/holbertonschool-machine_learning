{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    # Takes in input from the user and prints a (Filler) response\n",
    "* If the input is exit, quit, goodbye or bye in any case sensitive way, the program must print:\n",
    "\n",
    "\"A: Goodbye\"\n",
    "and exit"
   ],
   "id": "4f8e298be44c458b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from supervised_learning.qa_bot.main import reference\n",
    "\n",
    "while True:    \n",
    "    A = input(\"Q: \")\n",
    "\n",
    "    exit = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
    "\n",
    "    if A.lower() in exit:\n",
    "        print(\"A: Goodbye\")\n",
    "        break\n",
    "    else:   \n",
    "        print(\"A:\")\n",
    "        \n"
   ],
   "id": "3572d1b12dc996ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### That was the first task of this variety.\n",
    "* For the second one we take the previous one and basically use the reference text to answer the questions.\n",
    "- If the answer is not to be found: \"Sorry, I do not understand your question\""
   ],
   "id": "a451620e1231e25c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer"
   ],
   "id": "454229b8fb77e4f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def base():\n",
    "    while True:    \n",
    "        global A\n",
    "        A = input(\"Q: \")\n",
    "        exit = [\"exit\", \"quit\", \"goodbye\", \"bye\"]\n",
    "\n",
    "        if A.lower() in exit:\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        else:   \n",
    "            answer_loop(reference)\n",
    "\n",
    "\n",
    "def answer_loop(reference):\n",
    "    \"\"\"\n",
    "    Answers questions based on a reference text\n",
    "    \n",
    "    Parameters:\n",
    "        reference -> reference text given\n",
    "    \n",
    "    Returns:\n",
    "        the answer or \"Sorry I do not understand your question\"\n",
    "    \"\"\"\n",
    "    bert = hub.load(\"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    \n",
    "    # Tokenizes the questions and references\n",
    "    tokens_q = tokenizer(A, return_tensors=\"tf\", truncation=True, padding=True)[\"input_ids\"]\n",
    "    tokens_r = tokenizer(reference, return_tensors=\"tf\", truncation=True, padding=True)[\"input_ids\"]\n",
    "    \n",
    "    # Combine the questiona and reference documents into one\n",
    "    combine = tf.concat([tokens_q[:, : 1024], tokens_r[:, 1024 :]], axis=-1)\n",
    "    \n",
    "    # Make the predictions\n",
    "    outputs = bert(combine)[0]\n",
    "    logits = tf.argmax(outputs, axis=1).numpy()\n",
    "    \n",
    "    # Get the answer based on predicted idx\n",
    "    if logits[0] < len(tokens_q[0]):\n",
    "        return tokenizer.decode([tokens_r[0][logits[0]]])\n",
    "    else:\n",
    "        return None\n",
    "    "
   ],
   "id": "5bbbde5215f1e958",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
