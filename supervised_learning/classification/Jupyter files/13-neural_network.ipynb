{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adds the very convoluted Gradient Descent function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "714b91711602f98b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "A neural network with one hidden layer performing binary classification\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network with one hidden layer performing binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, nodes):\n",
    "        \"\"\"\n",
    "        nx is the number of input features\n",
    "\n",
    "        nodes is the number of nodes in the hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(nx, int):\n",
    "            raise TypeError('nx must be an integer')\n",
    "        if nx < 1:\n",
    "            raise ValueError('nx must be a positive integer')\n",
    "        if not isinstance(nodes, int):\n",
    "            raise TypeError('nodes must be an integer')\n",
    "        if nodes < 1:\n",
    "            raise ValueError('nodes must be a positive integer')\n",
    "\n",
    "        self.__W1 = np.random.randn(nodes, nx)\n",
    "        self.__W2 = np.random.randn(1, nodes)\n",
    "        self.__b1 = np.zeros((nodes, 1))\n",
    "        self.__b2 = 0\n",
    "        self.__A1 = 0\n",
    "        self.__A2 = 0\n",
    "        \n",
    "    @property\n",
    "    def W1(self):\n",
    "        return self.__W1\n",
    "    \n",
    "    @property\n",
    "    def W2(self):\n",
    "        return self.__W2\n",
    "    \n",
    "    @property\n",
    "    def b1(self):\n",
    "        return self.__b1\n",
    "    \n",
    "    @property\n",
    "    def b2(self):\n",
    "        return self.__b2\n",
    "    \n",
    "    @property\n",
    "    def A1(self):\n",
    "        return self.__A1\n",
    "    \n",
    "    @property\n",
    "    def A2(self):\n",
    "        return self.__A2\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the forward propagation of the neural network\n",
    "        :param X: array with shape (nx, m) with input data\n",
    "        nx is the number of input features\n",
    "        m is the number of examples\n",
    "        \"\"\"\n",
    "        z1 = np.matmul(self.__W1, X) + self.__b1\n",
    "        self.__A1 = 1 / (1 + np.exp(-z1))\n",
    "        z2 = np.matmul(self.__W2, self.__A1) + self.__b2\n",
    "        self.__A2 = 1 / (1 + np.exp(-z2))\n",
    "        return self.__A1, self.__A2\n",
    "\n",
    "    def cost(self, Y, A):\n",
    "        \"\"\"\n",
    "        Calculates the cost of the model using logistic regression\n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param A: array with shape (1, m) with activated  outputs\n",
    "        for each example\n",
    "        To avoid division by zero errors, we will use\n",
    "        1.0000001 - A instead of 1 - A\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        m_loss = np.sum(( Y * np.log(A) + (1 - Y) * np.log((1.0000001 - A)) ))\n",
    "        costs = (1 / m) *  (-m_loss)\n",
    "        return costs\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "         \"\"\"\n",
    "         evaluates the neural network's predictions\n",
    "         :param X: array with shape (nx, m) with input data\n",
    "         nx is the number of input features in the neuron\n",
    "         m is the number of examples\n",
    "         :param Y: array with shape (1, m) with correct labels for input data\n",
    "         \"\"\"\n",
    "         A1, A2 = self.forward_prop(X)\n",
    "         cost = self.cost(Y, A2)\n",
    "         predictions = np.where(A2 >= 0.5, 1, 0)\n",
    "         return predictions, cost\n",
    "\n",
    "    def gradient_descent(self, X, Y, A1, A2, alpha=0.05):\n",
    "        \"\"\"\n",
    "        :param X: array with shape (nx, m) with the input data\n",
    "        nx is the number of input features in the\n",
    "        m is the number of examples\n",
    "        \n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param A1: output of hidden layer\n",
    "        :param A2: predicted output\n",
    "        :param alpha: learning rate\n",
    "        \n",
    "        Updates the private attributes __W1, __b1, __W2, and __b2\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        dz2 = (A2 - Y)\n",
    "        d__W2 = (1 / m) * (np.matmul(dz2, A1.transpose()))\n",
    "        d__b2 = (1 / m) * (np.sum(dz2, axis=1, keepdims=True))\n",
    "\n",
    "        dz1 = (np.matmul(self.W2.transpose(), dz2)) * (A1 * (1 - A1))\n",
    "        d__W1 = (1 / m) * (np.matmul(dz1, X.transpose()))\n",
    "        d__b1 = (1 / m) * (np.sum(dz1, axis=1, keepdims=True))\n",
    "\n",
    "        self.__W2 = self.W2 - (alpha * d__W2)\n",
    "        self.__b2 = self.b2 - (alpha * d__b2)\n",
    "        self.__W1 = self.W1 - (alpha * d__W1)\n",
    "        self.__b1 = self.b1 - (alpha * d__b1)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T16:17:47.541035Z",
     "start_time": "2024-03-28T16:17:47.347745Z"
    }
   },
   "id": "c3df9c4632ec011e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]]\n",
      "[[ 0.003193  ]\n",
      " [-0.01080922]\n",
      " [-0.01045412]]\n",
      "[[ 1.06583858 -1.06149724 -1.79864091]]\n",
      "[[0.15552509]]\n"
     ]
    }
   ],
   "source": [
    "# Main Func\n",
    "\n",
    "NN = NeuralNetwork\n",
    "\n",
    "lib_train = np.load('../data/Binary_Train.npz')\n",
    "X_3D, Y = lib_train['X'], lib_train['Y']\n",
    "X = X_3D.reshape((X_3D.shape[0], -1)).T\n",
    "\n",
    "np.random.seed(0)\n",
    "nn = NN(X.shape[0], 3)\n",
    "A1, A2 = nn.forward_prop(X)\n",
    "nn.gradient_descent(X, Y, A1, A2, 0.5)\n",
    "print(nn.W1)\n",
    "print(nn.b1)\n",
    "print(nn.W2)\n",
    "print(nn.b2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T16:18:15.407398Z",
     "start_time": "2024-03-28T16:18:15.172483Z"
    }
   },
   "id": "a1853fc743fdae6d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7194ab6cc0fefc15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
