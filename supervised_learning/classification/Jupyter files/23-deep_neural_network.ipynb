{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# graph time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fec2b83307afd38"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "The Neural Network Deepens\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    The Deep Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, layers):\n",
    "        \"\"\"\n",
    "        Initializes the Deep Neural Network\n",
    "        \"\"\"\n",
    "        if not isinstance(nx, int):\n",
    "            raise TypeError('nx must be an integer')\n",
    "        if nx < 1:\n",
    "            raise ValueError('nx must be a positive integer')\n",
    "        if not isinstance(layers, list) or len(layers) == 0:\n",
    "            raise TypeError('layers must be a list of positive integers')\n",
    "        \n",
    "        weights = {}\n",
    "        previous = nx\n",
    "        \n",
    "        for index, layer in enumerate(layers, 1):\n",
    "            if not isinstance(layer, int) or layer < 1:\n",
    "                raise TypeError('layers must be a list of positive integers')\n",
    "            \n",
    "            weights[\"b{}\".format(index)] = np.zeros((layer, 1))      \n",
    "            weights[\"W{}\".format(index)] = (np.random.randn(layer, previous) *\n",
    "                                            np.sqrt(2 / previous))\n",
    "            previous = layer\n",
    "            \n",
    "        self.__L = len(layers)\n",
    "        self.__cache = {}\n",
    "        self.__weights = weights\n",
    "        \n",
    "    @property\n",
    "    def L(self):\n",
    "        return self.__L\n",
    "    \n",
    "    @property\n",
    "    def cache(self):\n",
    "        return self.__cache\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the forward propagation of the neural network\n",
    "        :param X: array with shape (nx, m) with input data\n",
    "        nx is the number of input features\n",
    "        m is the number of examples\n",
    "        \"\"\"\n",
    "        self.__cache[\"A0\"] = X\n",
    "        \n",
    "        for index in range(self.L):\n",
    "            W = self.weights[\"W{}\".format(index + 1)]\n",
    "            b = self.weights[\"b{}\".format(index + 1)]\n",
    "            \n",
    "            z = np.matmul(W, self.cache[\"A{}\".format(index)]) + b\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            self.__cache[\"A{}\".format(index + 1)] = a\n",
    "        return a, self.cache\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "        \"\"\"\n",
    "        Calculates the cost of the model using logistic regression\n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param A: array with shape (1, m) with activated  outputs\n",
    "        for each example\n",
    "        To avoid division by zero errors, we will use\n",
    "        1.0000001 - A instead of 1 - A\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        m_loss = np.sum(( Y * np.log(A) + (1 - Y) * np.log((1.0000001 - A)) ))\n",
    "        costs = (1 / m) *  (-m_loss)\n",
    "        return costs\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Evaluates the deep neural network\n",
    "        \"\"\"\n",
    "        A, cache = self.forward_prop(X)\n",
    "        cost = self.cost(Y, A)\n",
    "        predictions = np.where(A >= 0.5, 1, 0)\n",
    "        return predictions, cost\n",
    "    \n",
    "    def gradient_descent(self, Y, cache, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculates the Gradient Descent of one pass\n",
    "        \n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param cache: dictionary with intermediary values of the network\n",
    "        :param alpha: learning rate\n",
    "        \n",
    "        updates the private attributes __weights\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        back = {}\n",
    "\n",
    "        for index in range(self.L, 0, -1):\n",
    "\n",
    "            A = cache[\"A{}\".format(index - 1)]\n",
    "            if index == self.L:\n",
    "                back[\"dz{}\".format(index)] = (cache[\"A{}\".format(index)] - Y)\n",
    "            else:\n",
    "                dz_prev = back[\"dz{}\".format(index + 1)]\n",
    "                A_current = cache[\"A{}\".format(index)]\n",
    "                back[\"dz{}\".format(index)] = (\n",
    "                    np.matmul(W_prev.transpose(), dz_prev) *\n",
    "                    (A_current * (1 - A_current)))\n",
    "\n",
    "            dz = back[\"dz{}\".format(index)]\n",
    "            dW = (1 / m) * (np.matmul(dz, A.transpose()))\n",
    "            db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            W_prev = self.weights[\"W{}\".format(index)]\n",
    "\n",
    "            self.__weights[\"W{}\".format(index)] = (\n",
    "                self.weights[\"W{}\".format(index)] - (alpha * dW))\n",
    "            self.__weights[\"b{}\".format(index)] = (\n",
    "                self.weights[\"b{}\".format(index)] - (alpha * db))\n",
    "\n",
    "    def train(self, X, Y, iterations=5000, alpha=0.05, verbose=True,\n",
    "              graph=True, step=100):\n",
    "        \"\"\"\n",
    "        Trains the neuron and updates weights and cache\n",
    "        \n",
    "        :param X: array with input data, shape (nx, m)\n",
    "        nx is the number of input samples\n",
    "        m is the number of examples\n",
    "        \n",
    "        :param Y: array with shape (1, m) with the correct labels\n",
    "        \n",
    "        iterations: number of iterations\n",
    "        if iterations is not an integer, raise a TypeError with the exception iterations must be an integer\n",
    "        if iterations is not positive, raise a ValueError with the exception iterations must be a positive integer\n",
    "        \n",
    "        :param alpha: learning rate\n",
    "        if alpha is not a float, raise a TypeError with the exception alpha must be a float\n",
    "        if alpha is not positive, raise a ValueError with the exception alpha must be positive\n",
    "        \n",
    "        :param verbose: whether to or not print the training progress\n",
    "        \n",
    "        :param graph: whether to print the graph\n",
    "        \n",
    "        :param step: step size for gradient descent\n",
    "        \"\"\"\n",
    "        if type(iterations) is not int:\n",
    "            raise TypeError(\"iterations must be an integer\")\n",
    "        if iterations <= 0:\n",
    "            raise ValueError(\"iterations must be a positive integer\")\n",
    "        if type(alpha) is not float:\n",
    "            raise TypeError(\"alpha must be a float\")\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"alpha must be positive\")\n",
    "        if verbose or graph:\n",
    "            if type(step) is not int:\n",
    "                raise TypeError(\"step must be an integer\")\n",
    "            if step <= 0 or step > iterations:\n",
    "                raise ValueError(\"step must be positive and <= iterations\")\n",
    "        if graph:\n",
    "            import matplotlib.pyplot as plt\n",
    "            x_points = np.arange(0, iterations + 1, step)\n",
    "            points = []\n",
    "        for itr in range(iterations):\n",
    "            A, cache = self.forward_prop(X)\n",
    "            if verbose and (itr % step) == 0:\n",
    "                cost = self.cost(Y, A)\n",
    "                print(\"Cost after \" + str(itr) + \" iterations: \" + str(cost))\n",
    "            if graph and (itr % step) == 0:\n",
    "                cost = self.cost(Y, A)\n",
    "                points.append(cost)\n",
    "            self.gradient_descent(Y, cache, alpha)\n",
    "        itr += 1\n",
    "        if verbose:\n",
    "            cost = self.cost(Y, A)\n",
    "            print(\"Cost after \" + str(itr) + \" iterations: \" + str(cost))\n",
    "        if graph:\n",
    "            cost = self.cost(Y, A)\n",
    "            points.append(cost)\n",
    "            y_points = np.asarray(points)\n",
    "            plt.plot(x_points, y_points, 'b')\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"cost\")\n",
    "            plt.title(\"Training Cost\")\n",
    "            plt.show()\n",
    "        return self.evaluate(X, Y)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:43:49.163553Z",
     "start_time": "2024-04-03T17:43:49.142382Z"
    }
   },
   "id": "4e0c2b3f4c4e4fd9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 0.6958649419170609\n",
      "Cost after 100 iterations: 0.6444304786060048\n"
     ]
    }
   ],
   "source": [
    "Deep = DeepNeuralNetwork\n",
    "\n",
    "lib_train = np.load('../data/Binary_Train.npz')\n",
    "X_train_3D, Y_train = lib_train['X'], lib_train['Y']\n",
    "X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "lib_dev = np.load('../data/Binary_Dev.npz')\n",
    "X_dev_3D, Y_dev = lib_dev['X'], lib_dev['Y']\n",
    "X_dev = X_dev_3D.reshape((X_dev_3D.shape[0], -1)).T\n",
    "\n",
    "np.random.seed(0)\n",
    "deep = Deep(X_train.shape[0], [5, 3, 1])\n",
    "A, cost = deep.train(X_train, Y_train)\n",
    "accuracy = np.sum(A == Y_train) / Y_train.shape[1] * 100\n",
    "print(\"Train cost:\", cost)\n",
    "print(\"Train accuracy: {}%\".format(accuracy))\n",
    "A, cost = deep.evaluate(X_dev, Y_dev)\n",
    "accuracy = np.sum(A == Y_dev) / Y_dev.shape[1] * 100\n",
    "print(\"Dev cost:\", cost)\n",
    "print(\"Dev accuracy: {}%\".format(accuracy))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    fig.add_subplot(10, 10, i + 1)\n",
    "    plt.imshow(X_dev_3D[i])\n",
    "    plt.title(A[0, i])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-03T17:44:22.231112Z"
    }
   },
   "id": "b015543dc4f11426"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b93d378c4d0daf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
