{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adds the train function.\n",
    "Very Easy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e820713be9c5823"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "The Neural Network Deepens\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    The Deep Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, layers):\n",
    "        \"\"\"\n",
    "        Initializes the Deep Neural Network\n",
    "        \"\"\"\n",
    "        if not isinstance(nx, int):\n",
    "            raise TypeError('nx must be an integer')\n",
    "        if nx < 1:\n",
    "            raise ValueError('nx must be a positive integer')\n",
    "        if not isinstance(layers, list) or len(layers) == 0:\n",
    "            raise TypeError('layers must be a list of positive integers')\n",
    "        \n",
    "        weights = {}\n",
    "        previous = nx\n",
    "        \n",
    "        for index, layer in enumerate(layers, 1):\n",
    "            if not isinstance(layer, int) or layer < 1:\n",
    "                raise TypeError('layers must be a list of positive integers')\n",
    "            \n",
    "            weights[\"b{}\".format(index)] = np.zeros((layer, 1))      \n",
    "            weights[\"W{}\".format(index)] = (np.random.randn(layer, previous) *\n",
    "                                            np.sqrt(2 / previous))\n",
    "            previous = layer\n",
    "            \n",
    "        self.__L = len(layers)\n",
    "        self.__cache = {}\n",
    "        self.__weights = weights\n",
    "        \n",
    "    @property\n",
    "    def L(self):\n",
    "        return self.__L\n",
    "    \n",
    "    @property\n",
    "    def cache(self):\n",
    "        return self.__cache\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the forward propagation of the neural network\n",
    "        :param X: array with shape (nx, m) with input data\n",
    "        nx is the number of input features\n",
    "        m is the number of examples\n",
    "        \"\"\"\n",
    "        self.__cache[\"A0\"] = X\n",
    "        \n",
    "        for index in range(self.L):\n",
    "            W = self.weights[\"W{}\".format(index + 1)]\n",
    "            b = self.weights[\"b{}\".format(index + 1)]\n",
    "            \n",
    "            z = np.matmul(W, self.cache[\"A{}\".format(index)]) + b\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            self.__cache[\"A{}\".format(index + 1)] = a\n",
    "        return a, self.cache\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "        \"\"\"\n",
    "        Calculates the cost of the model using logistic regression\n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param A: array with shape (1, m) with activated  outputs\n",
    "        for each example\n",
    "        To avoid division by zero errors, we will use\n",
    "        1.0000001 - A instead of 1 - A\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        m_loss = np.sum(( Y * np.log(A) + (1 - Y) * np.log((1.0000001 - A)) ))\n",
    "        costs = (1 / m) *  (-m_loss)\n",
    "        return costs\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Evaluates the deep neural network\n",
    "        \"\"\"\n",
    "        A, cache = self.forward_prop(X)\n",
    "        cost = self.cost(Y, A)\n",
    "        predictions = np.where(A >= 0.5, 1, 0)\n",
    "        return predictions, cost\n",
    "    \n",
    "    def gradient_descent(self, Y, cache, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculates the Gradient Descent of one pass\n",
    "        \n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param cache: dictionary with intermediary values of the network\n",
    "        :param alpha: learning rate\n",
    "        \n",
    "        updates the private attributes __weights\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        back = {}\n",
    "\n",
    "        for index in range(self.L, 0, -1):\n",
    "\n",
    "            A = cache[\"A{}\".format(index - 1)]\n",
    "            if index == self.L:\n",
    "                back[\"dz{}\".format(index)] = (cache[\"A{}\".format(index)] - Y)\n",
    "            else:\n",
    "                dz_prev = back[\"dz{}\".format(index + 1)]\n",
    "                A_current = cache[\"A{}\".format(index)]\n",
    "                back[\"dz{}\".format(index)] = (\n",
    "                    np.matmul(W_prev.transpose(), dz_prev) *\n",
    "                    (A_current * (1 - A_current)))\n",
    "\n",
    "            dz = back[\"dz{}\".format(index)]\n",
    "            dW = (1 / m) * (np.matmul(dz, A.transpose()))\n",
    "            db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            W_prev = self.weights[\"W{}\".format(index)]\n",
    "\n",
    "            self.__weights[\"W{}\".format(index)] = (\n",
    "                self.weights[\"W{}\".format(index)] - (alpha * dW))\n",
    "            self.__weights[\"b{}\".format(index)] = (\n",
    "                self.weights[\"b{}\".format(index)] - (alpha * db))\n",
    "\n",
    "    def train(self, X, Y, iterations=5000, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Trains the neuron and updates weights and cache\n",
    "        \n",
    "        :param X: array with input data, shape (nx, m)\n",
    "        nx is the number of input samples\n",
    "        m is the number of examples\n",
    "        \n",
    "        :param Y: array with shape (1, m) with the correct labels\n",
    "        \n",
    "        iterations: number of iterations\n",
    "        if iterations is not an integer, raise a TypeError with the exception iterations must be an integer\n",
    "        if iterations is not positive, raise a ValueError with the exception iterations must be a positive integer\n",
    "        \n",
    "        :param alpha: learning rate\n",
    "        if alpha is not a float, raise a TypeError with the exception alpha must be a float\n",
    "        if alpha is not positive, raise a ValueError with the exception alpha must be positive\n",
    "        \"\"\"\n",
    "        if type(iterations) is not int:\n",
    "            raise TypeError(\"iterations must be an integer\")\n",
    "        if iterations <= 0:\n",
    "            raise ValueError(\"iterations must be a positive integer\")\n",
    "        if type(alpha) is not float:\n",
    "            raise TypeError(\"alpha must be a float\")\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"alpha must be positive\")\n",
    "\n",
    "        for itr in range(iterations):\n",
    "            A, cache = self.forward_prop(X)\n",
    "            self.gradient_descent(Y, cache, alpha)\n",
    "\n",
    "        return self.evaluate(X, Y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bde9ae5e1963a2ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
