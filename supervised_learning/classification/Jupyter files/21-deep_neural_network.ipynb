{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adding the cringe Gradient Descent Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6304a30c9d8f452"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "The Neural Network Deepens\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    The Deep Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, layers):\n",
    "        \"\"\"\n",
    "        Initializes the Deep Neural Network\n",
    "        \"\"\"\n",
    "        if not isinstance(nx, int):\n",
    "            raise TypeError('nx must be an integer')\n",
    "        if nx < 1:\n",
    "            raise ValueError('nx must be a positive integer')\n",
    "        if not isinstance(layers, list) or len(layers) == 0:\n",
    "            raise TypeError('layers must be a list of positive integers')\n",
    "        \n",
    "        weights = {}\n",
    "        previous = nx\n",
    "        \n",
    "        for index, layer in enumerate(layers, 1):\n",
    "            if not isinstance(layer, int) or layer < 1:\n",
    "                raise TypeError('layers must be a list of positive integers')\n",
    "            \n",
    "            weights[\"b{}\".format(index)] = np.zeros((layer, 1))      \n",
    "            weights[\"W{}\".format(index)] = (np.random.randn(layer, previous) *\n",
    "                                            np.sqrt(2 / previous))\n",
    "            previous = layer\n",
    "            \n",
    "        self.__L = len(layers)\n",
    "        self.__cache = {}\n",
    "        self.__weights = weights\n",
    "        \n",
    "    @property\n",
    "    def L(self):\n",
    "        return self.__L\n",
    "    \n",
    "    @property\n",
    "    def cache(self):\n",
    "        return self.__cache\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Calculates the forward propagation of the neural network\n",
    "        :param X: array with shape (nx, m) with input data\n",
    "        nx is the number of input features\n",
    "        m is the number of examples\n",
    "        \"\"\"\n",
    "        self.__cache[\"A0\"] = X\n",
    "        \n",
    "        for index in range(self.L):\n",
    "            W = self.weights[\"W{}\".format(index + 1)]\n",
    "            b = self.weights[\"b{}\".format(index + 1)]\n",
    "            \n",
    "            z = np.matmul(W, self.cache[\"A{}\".format(index)]) + b\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            self.__cache[\"A{}\".format(index + 1)] = a\n",
    "        return a, self.cache\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "        \"\"\"\n",
    "        Calculates the cost of the model using logistic regression\n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param A: array with shape (1, m) with activated  outputs\n",
    "        for each example\n",
    "        To avoid division by zero errors, we will use\n",
    "        1.0000001 - A instead of 1 - A\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        m_loss = np.sum(( Y * np.log(A) + (1 - Y) * np.log((1.0000001 - A)) ))\n",
    "        costs = (1 / m) *  (-m_loss)\n",
    "        return costs\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Evaluates the deep neural network\n",
    "        \"\"\"\n",
    "        A, cache = self.forward_prop(X)\n",
    "        cost = self.cost(Y, A)\n",
    "        predictions = np.where(A >= 0.5, 1, 0)\n",
    "        return predictions, cost\n",
    "    \n",
    "    def gradient_descent(self, Y, cache, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculates the Gradient Descent of one pass\n",
    "        \n",
    "        :param Y: array with shape (1, m) with correct labels for input data\n",
    "        :param cache: dictionary with intermediary values of the network\n",
    "        :param alpha: learning rate\n",
    "        \n",
    "        updates the private attributes __weights\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        back = {}\n",
    "\n",
    "        for index in range(self.L, 0, -1):\n",
    "\n",
    "            A = cache[\"A{}\".format(index - 1)]\n",
    "            if index == self.L:\n",
    "                back[\"dz{}\".format(index)] = (cache[\"A{}\".format(index)] - Y)\n",
    "            else:\n",
    "                dz_prev = back[\"dz{}\".format(index + 1)]\n",
    "                A_current = cache[\"A{}\".format(index)]\n",
    "                back[\"dz{}\".format(index)] = (\n",
    "                    np.matmul(W_prev.transpose(), dz_prev) *\n",
    "                    (A_current * (1 - A_current)))\n",
    "\n",
    "            dz = back[\"dz{}\".format(index)]\n",
    "            dW = (1 / m) * (np.matmul(dz, A.transpose()))\n",
    "            db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            W_prev = self.weights[\"W{}\".format(index)]\n",
    "\n",
    "            self.__weights[\"W{}\".format(index)] = (\n",
    "                self.weights[\"W{}\".format(index)] - (alpha * dW))\n",
    "            self.__weights[\"b{}\".format(index)] = (\n",
    "                self.weights[\"b{}\".format(index)] - (alpha * db))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:34:03.283343Z",
     "start_time": "2024-04-03T17:34:03.104429Z"
    }
   },
   "id": "2d0d6b010882359f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b1': array([[-1.01835520e-03],\n",
      "       [-1.22929756e-04],\n",
      "       [ 9.25521878e-05],\n",
      "       [ 1.07730873e-04],\n",
      "       [ 2.29014796e-04]]), 'W1': array([[ 0.0890981 ,  0.02021099,  0.04943373, ...,  0.02632982,\n",
      "         0.03090699, -0.06775582],\n",
      "       [ 0.02408701,  0.00749784,  0.02672082, ...,  0.00484894,\n",
      "        -0.00227857,  0.00399625],\n",
      "       [ 0.04295829, -0.04238217, -0.05110231, ..., -0.00364861,\n",
      "         0.01571416, -0.05446546],\n",
      "       [ 0.05361891, -0.05984585, -0.09117898, ..., -0.03094292,\n",
      "        -0.01925805, -0.06308145],\n",
      "       [-0.01667953, -0.04216413,  0.06239623, ..., -0.02024521,\n",
      "        -0.05159656, -0.02373981]]), 'b2': array([[-0.00055419],\n",
      "       [ 0.00032369],\n",
      "       [ 0.0007201 ]]), 'W2': array([[ 0.4586347 ,  0.55968571, -1.22435332, -0.09516874,  0.57668454],\n",
      "       [-0.16209305,  0.06902405, -0.9460547 , -0.30329296,  1.15722071],\n",
      "       [-0.49595566, -0.91068385,  0.09382566,  0.49948968,  0.75647764]]), 'b3': array([[0.00659936]]), 'W3': array([[-0.41262664,  0.18889024,  0.44717929]])}\n"
     ]
    }
   ],
   "source": [
    "# Main func\n",
    "\n",
    "Deep = DeepNeuralNetwork\n",
    "\n",
    "lib_train = np.load('../data/Binary_Train.npz')\n",
    "X_3D, Y = lib_train['X'], lib_train['Y']\n",
    "X = X_3D.reshape((X_3D.shape[0], -1)).T\n",
    "\n",
    "np.random.seed(0)\n",
    "deep = Deep(X.shape[0], [5, 3, 1])\n",
    "A, cache = deep.forward_prop(X)\n",
    "deep.gradient_descent(Y, cache, 0.5)\n",
    "print(deep.weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:34:24.092253Z",
     "start_time": "2024-04-03T17:34:23.877775Z"
    }
   },
   "id": "b94e4c2b5b3a1628",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b99077395fde87aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
